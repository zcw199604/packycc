use super::Segment;
use crate::config::{InputData, TranscriptEntry};
use std::fs;
use std::io::{BufRead, BufReader};
use std::path::Path;

/// Token ä½¿ç”¨ä¿¡æ¯
struct TokenUsage {
    // ç”¨äºè´¹ç”¨è®¡ç®—ï¼ˆç´¯åŠ å€¼ï¼‰
    total_input_tokens: u32,
    total_cache_read_tokens: u32,
    total_cache_write_tokens: u32,
    total_output_tokens: u32,
    // ç”¨äºä¸Šä¸‹æ–‡ä½¿ç”¨ç‡ï¼ˆæœ€åä¸€æ¡è®°å½•ï¼‰
    last_context_tokens: u32,
}

/// æ¨¡å‹å®šä»·ï¼ˆæ¯ç™¾ä¸‡ tokenï¼‰
struct ModelPricing {
    input: f64,
    output: f64,
    cache_read: f64,
    cache_write: f64,
}

/// æ ¹æ®æ¨¡å‹åç§°è·å–ä¸Šä¸‹æ–‡ä¸Šé™
fn get_context_limit(model_name: &str) -> u32 {
    if model_name.contains("1M") {
        1_000_000
    } else {
        200_000
    }
}

/// æ ¹æ®æ¨¡å‹åç§°å’Œ token ä½¿ç”¨é‡è·å–å®šä»·
fn get_model_pricing(model_name: &str, total_input_tokens: u32) -> ModelPricing {
    if model_name.contains("opus") || model_name.contains("Opus") {
        // Opus 4.5
        ModelPricing {
            input: 5.0,
            output: 25.0,
            cache_read: 0.5,
            cache_write: 6.25,
        }
    } else if model_name.contains("haiku") || model_name.contains("Haiku") {
        // Haiku
        ModelPricing {
            input: 0.25,
            output: 1.25,
            cache_read: 0.03,
            cache_write: 0.30,
        }
    } else if model_name.contains("1M") && total_input_tokens > 200_000 {
        // Sonnet 4.5 1M (>200K ä»·æ ¼ç¿»å€)
        ModelPricing {
            input: 6.0,
            output: 22.5,
            cache_read: 0.60,
            cache_write: 7.50,
        }
    } else {
        // Sonnet 4.5 é»˜è®¤ (â‰¤200K)
        ModelPricing {
            input: 3.0,
            output: 15.0,
            cache_read: 0.30,
            cache_write: 3.75,
        }
    }
}

/// è®¡ç®—æ€»è´¹ç”¨
fn calculate_cost(usage: &TokenUsage, pricing: &ModelPricing) -> f64 {
    let million = 1_000_000.0;
    (usage.total_input_tokens as f64 / million) * pricing.input
        + (usage.total_cache_read_tokens as f64 / million) * pricing.cache_read
        + (usage.total_cache_write_tokens as f64 / million) * pricing.cache_write
        + (usage.total_output_tokens as f64 / million) * pricing.output
}

pub struct UsageSegment {
    enabled: bool,
}

impl UsageSegment {
    pub fn new(enabled: bool) -> Self {
        Self { enabled }
    }
}

impl Segment for UsageSegment {
    fn render(&self, input: &InputData) -> String {
        if !self.enabled {
            return String::new();
        }

        let usage = parse_transcript_usage(&input.transcript_path);
        let context_limit = get_context_limit(&input.model.display_name);
        // ä½¿ç”¨æœ€åä¸€æ¡è®°å½•çš„ä¸Šä¸‹æ–‡ token è®¡ç®—ä½¿ç”¨ç‡
        let context_used_token = usage.last_context_tokens;
        let context_used_rate = (context_used_token as f64 / context_limit as f64) * 100.0;

        // æ ¼å¼åŒ– token æ˜¾ç¤ºï¼ˆå½“å‰/æ€»é‡ï¼‰
        let current_display = format_token_count(context_used_token);
        let limit_display = format_token_count(context_limit);

        // ç”Ÿæˆè¿›åº¦æ¡ ğŸŸ©ğŸŸ©â¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œâ¬œ
        let bar_width = 10;
        let filled = ((context_used_rate / 100.0) * bar_width as f64).round() as usize;
        let filled = filled.min(bar_width); // ç¡®ä¿ä¸è¶…è¿‡æ€»å®½åº¦
        let empty = bar_width - filled;
        let progress_bar = format!("{}{}", "ğŸŸ©".repeat(filled), "â¬œ".repeat(empty));

        // è®¡ç®—è´¹ç”¨ï¼ˆä½¿ç”¨ç´¯åŠ çš„ tokenï¼‰
        let pricing = get_model_pricing(&input.model.display_name, context_used_token);
        let cost = calculate_cost(&usage, &pricing);

        format!(
            "{} {:.1}% ({}/{}) | ${:.2}",
            progress_bar, context_used_rate, current_display, limit_display, cost
        )
    }

    fn enabled(&self) -> bool {
        self.enabled
    }
}

/// æ ¼å¼åŒ– token æ•°é‡æ˜¾ç¤º
fn format_token_count(tokens: u32) -> String {
    if tokens >= 1_000_000 {
        format!("{:.1}M", tokens as f64 / 1_000_000.0)
    } else if tokens >= 1000 {
        format!("{:.1}K", tokens as f64 / 1000.0)
    } else {
        tokens.to_string()
    }
}

/// è§£æ transcript æ–‡ä»¶è·å– token ä½¿ç”¨ä¿¡æ¯
/// - ç´¯åŠ æ‰€æœ‰ token ç”¨äºè´¹ç”¨è®¡ç®—
/// - æœ€åä¸€æ¡è®°å½•çš„ä¸Šä¸‹æ–‡ token ç”¨äºä½¿ç”¨ç‡è®¡ç®—
fn parse_transcript_usage<P: AsRef<Path>>(transcript_path: P) -> TokenUsage {
    let file = match fs::File::open(&transcript_path) {
        Ok(file) => file,
        Err(_) => {
            return TokenUsage {
                total_input_tokens: 0,
                total_cache_read_tokens: 0,
                total_cache_write_tokens: 0,
                total_output_tokens: 0,
                last_context_tokens: 0,
            }
        }
    };

    let reader = BufReader::new(file);
    let lines: Vec<String> = reader
        .lines()
        .collect::<Result<Vec<_>, _>>()
        .unwrap_or_default();

    // ç´¯åŠ æ‰€æœ‰ tokenï¼ˆç”¨äºè´¹ç”¨è®¡ç®—ï¼‰
    let mut total_input_tokens: u32 = 0;
    let mut total_cache_read_tokens: u32 = 0;
    let mut total_cache_write_tokens: u32 = 0;
    let mut total_output_tokens: u32 = 0;

    // æœ€åä¸€æ¡è®°å½•çš„ä¸Šä¸‹æ–‡ tokenï¼ˆç”¨äºä½¿ç”¨ç‡è®¡ç®—ï¼‰
    let mut last_context_tokens: u32 = 0;

    for line in lines.iter() {
        let line = line.trim();
        if line.is_empty() {
            continue;
        }

        if let Ok(entry) = serde_json::from_str::<TranscriptEntry>(line) {
            if entry.r#type.as_deref() == Some("assistant") {
                if let Some(message) = &entry.message {
                    if let Some(usage) = &message.usage {
                        // ç´¯åŠ ç”¨äºè´¹ç”¨è®¡ç®—
                        total_input_tokens += usage.input_tokens;
                        total_cache_read_tokens += usage.cache_read_input_tokens;
                        total_cache_write_tokens += usage.cache_creation_input_tokens;
                        total_output_tokens += usage.output_tokens;

                        // æ›´æ–°æœ€åä¸€æ¡çš„ä¸Šä¸‹æ–‡ token
                        last_context_tokens = usage.input_tokens
                            + usage.cache_read_input_tokens
                            + usage.cache_creation_input_tokens;
                    }
                }
            }
        }
    }

    TokenUsage {
        total_input_tokens,
        total_cache_read_tokens,
        total_cache_write_tokens,
        total_output_tokens,
        last_context_tokens,
    }
}
